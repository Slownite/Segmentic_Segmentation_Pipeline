{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996477c5-bcf2-435c-88ca-0e62178f43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from SSP.process_voc import VOCSegmentationWithJointTransform, JointTransform\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e885ca6-b5e2-496b-ade3-c38d38a126f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/Documents/perso/project/SSP/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights, num_classes=21)\n",
    "model.load_state_dict(torch.load(\"weights/F_model_weights_pruned_0.4.pth\"), strict=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10dccf96-1355-495f-8a1f-d6e465ee0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a551b5c-8f6a-4347-9159-eb9e0ef44cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,                          # your PyTorch model\n",
    "    dummy_input,                    # example input\n",
    "    \"model.onnx\",                   # output file name\n",
    "    export_params=True,            # store weights\n",
    "    opset_version=11,              # ONNX version (11+ is widely supported)\n",
    "    do_constant_folding=True,      # optimize constants\n",
    "    input_names=['input'],         # name for input tensor\n",
    "    output_names=['output'],       # name for output tensor\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # support variable batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b056db88-60ba-4819-abdf-cff29f10809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model is valid.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298b2acc-5cd0-4039-b1ef-7780440dd09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 21, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Convert PyTorch tensor to NumPy array\n",
    "input_numpy = dummy_input.numpy()\n",
    "\n",
    "# Run inference\n",
    "outputs = ort_session.run(None, {\"input\": input_numpy})\n",
    "print(outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02560661-0c71-4f38-b528-1def9803112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VOCSegmentationWithJointTransform(\n",
    "    root='data',\n",
    "    year='2012',\n",
    "    image_set='train',\n",
    "    download=True,\n",
    "    joint_transform=JointTransform()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba922b2-0ab1-4424-8286-f4c976c3f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90780315-e830-49de-9458-e8b13c91e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "class VOCDataReader(CalibrationDataReader):\n",
    "    def __init__(self, dataloader, input_name):\n",
    "        self.dataloader = iter(dataloader)\n",
    "        self.input_name = input_name\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            images, _ = next(self.dataloader)\n",
    "            # Apply normalization\n",
    "            images = torch.stack([self.normalize(img) for img in images])\n",
    "            return {self.input_name: images.numpy()}\n",
    "        except StopIteration:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea98dbe-930c-4755-aad6-12d1e7db5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx = onnx.load(\"model.onnx\")\n",
    "input_name = model_onnx.graph.input[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2bf1267-c30a-447c-8c88-6cd5aa6fc991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. Or it will lead to bad performance on x64.\n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static, QuantType\n",
    "\n",
    "reader = VOCDataReader(loader, input_name)\n",
    "\n",
    "quantize_static(\n",
    "    model_input='model.onnx',\n",
    "    model_output='model_quantized.onnx',\n",
    "    calibration_data_reader=reader,\n",
    "    quant_format='QOperator',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    activation_type=QuantType.QInt8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b6051-eb0e-448f-a47a-75c9b096965b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
